{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append(\"../../src\")\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from example_lorenz import get_lorenz_data, generate_lorenz_data\n",
    "#from autoencoder import full_network\n",
    "#from training import create_feed_dictionary\n",
    "#from sindy_utils import sindy_simulate\n",
    "#from sindy_utils import sindy_library\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from shallowNet.shallowNet import shallowNet, DenseTranspose\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Flatten\n",
    "from typing import List, Tuple, Union\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "data_path = os.getcwd() + '/'\n",
    "save_name = 'model1'\n",
    "params = pickle.load(open(data_path + save_name + '_params.pkl', 'rb'))\n",
    "params['save_name'] = data_path + save_name\n",
    "\n",
    "t = np.arange(0,20,.01)\n",
    "z0 = np.array([[-8,7,27]])\n",
    "\n",
    "test_data = generate_lorenz_data(z0, t, params['input_dim'], linear=False, normalization=np.array([1/40,1/40,1/40]))\n",
    "test_data['x'] = test_data['x'].reshape((-1,params['input_dim']))\n",
    "test_data['dx'] = test_data['dx'].reshape((-1,params['input_dim']))\n",
    "test_data['z'] = test_data['z'].reshape((-1,params['latent_dim']))\n",
    "test_data['dz'] = test_data['dz'].reshape((-1,params['latent_dim']))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "test_data.keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['t', 'y_spatial', 'modes', 'x', 'dx', 'ddx', 'z', 'dz', 'ddz', 'sindy_coefficients'])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "test_data['x'][:,0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.0058125 , 0.05121063, 0.0912123 , ..., 0.14640952, 0.13924235,\n",
       "       0.1320653 ])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "z = tf.stack(test_data['x'][:16])\n",
    "latent_dim = 3\n",
    "poly_order = 2\n",
    "include_sine = True\n",
    "library = [tf.ones(tf.shape(z)[0])]\n",
    "\n",
    "def sindy_library_tf(z, latent_dim, poly_order, include_sine=False):\n",
    "    \"\"\"\n",
    "    !!!! TF2 - VERSION !!!!\n",
    "    \n",
    "    Build the SINDy library.\n",
    "\n",
    "    Arguments:\n",
    "        z - 2D tensorflow array of the snapshots on which to build the library. Shape is number of\n",
    "        time points by the number of state variables.\n",
    "        latent_dim - Integer, number of state variable in z.\n",
    "        poly_order - Integer, polynomial order to which to build the library. Max value is 5.\n",
    "        include_sine - Boolean, whether or not to include sine terms in the library. Default False.\n",
    "\n",
    "    Returns:\n",
    "        2D tensorflow array containing the constructed library. Shape is number of time points by\n",
    "        number of library functions. The number of library functions is determined by the number\n",
    "        of state variables of the input, the polynomial order, and whether or not sines are included.\n",
    "    \"\"\"\n",
    "\n",
    "    # add constant term to the set of equation\n",
    "    library = [tf.ones(tf.shape(z)[0])]\n",
    "\n",
    "    # add: x, y, z\n",
    "    for i in range(latent_dim):\n",
    "        library.append(z[:,i])\n",
    "\n",
    "    # add: xx + zz + yy + xy + xz + yz\n",
    "    if poly_order > 1:\n",
    "        print(\"call poly 1\")\n",
    "        for i in range(latent_dim):\n",
    "            for j in range(i,latent_dim):\n",
    "                library.append(z[:,i]*z[:,j])\n",
    "                #library.append(tf.math.multiply(z[:,i], z[:,j]))\n",
    "\n",
    "    # anologically to the comment above\n",
    "    if poly_order > 2:\n",
    "        print(\"call poly 2\")\n",
    "        for i in range(latent_dim):\n",
    "            for j in range(i,latent_dim):\n",
    "                for k in range(j,latent_dim):\n",
    "                    library.append(z[:,i]*z[:,j]*z[:,k])\n",
    "                    #library.append(tf.math.multiply(tf.math.multiply(z[:,i], z[:,j]), z[:,k]))\n",
    "\n",
    "    # anologically to the comment above\n",
    "    if poly_order > 3:\n",
    "        print(\"call poly 3\")\n",
    "        for i in range(latent_dim):\n",
    "            for j in range(i,latent_dim):\n",
    "                for k in range(j,latent_dim):\n",
    "                    for p in range(k,latent_dim):\n",
    "                        library.append(z[:,i]*z[:,j]*z[:,k]*z[:,p])\n",
    "                        #library.append(tf.math.multiply(tf.math.multiply(tf.math.multiply(z[:,i], z[:,j]), z[:,k]), z[:p]))\n",
    "\n",
    "    # anologically to the comment above\n",
    "    if poly_order > 4:\n",
    "        for i in range(latent_dim):\n",
    "            for j in range(i,latent_dim):\n",
    "                for k in range(j,latent_dim):\n",
    "                    for p in range(k,latent_dim):\n",
    "                        for q in range(p,latent_dim):\n",
    "                            library.append(z[:,i]*z[:,j]*z[:,k]*z[:,p]*z[:,q])\n",
    "                            #library.append(tf.math.multiply(tf.math.multiply(tf.math.multiply(tf.math.multiply(z[:,i], z[:,j]), z[:,k]), z[:p]), z[:q]))\n",
    "\n",
    "    # add: sin(x), sin(y), sin(z)\n",
    "    if include_sine:\n",
    "        for i in range(latent_dim):\n",
    "            library.append(np.sin(z[:,i]))\n",
    "            \n",
    "    return tf.convert_to_tensor(np.stack(library, axis = 1))\n",
    "\n",
    "r = sindy_library_tf(z, 3, 2, False)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "call poly 1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "Input(shape=(r.shape[0],)) "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 16) dtype=float32 (created by layer 'input_7')>"
      ]
     },
     "metadata": {},
     "execution_count": 111
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def build_network_layers(\n",
    "    encoding_layers_size = [32, 28], \n",
    "    layers_params = [{'l1' : 0.0001, 'l2' : 0.0, 'dropout' : 0.0}, {}],\n",
    "    layers_default_params = {'l1' : 0.0001, 'l2' : 0.0, 'dropout' : 0.0, 'activation' : \"tanh\"},\n",
    "    loss = \"mse\",\n",
    "    optimizer =  tf.keras.optimizers.Adam,\n",
    "    learning_rate = 0.0001,\n",
    "    metrics = None\n",
    "    ) -> tf.keras.Model:\n",
    "\n",
    "    # initialize input layer \n",
    "    input_size = encoding_layers_size[0]\n",
    "    inputs = Input(shape=(input_size,))\n",
    "\n",
    "    # add empty dicionaries to layers_params to corecponds to the encoding_layers_size size\n",
    "    for _ in range(len(encoding_layers_size) - len(layers_params)):\n",
    "        layers_params.append({})\n",
    "\n",
    "    # remove firs instance \n",
    "    encoding_layers_size = encoding_layers_size[1:]\n",
    "\n",
    "    # fill missing values in layers_params using default values \n",
    "    layers_params = [{**layers_default_params, **x} for x in layers_params]\n",
    "    \n",
    "    #initalize arrays to hold encoding and decoding layers \n",
    "    encoders = []\n",
    "    decoders = []\n",
    "\n",
    "    for l_size, l_param in zip(encoding_layers_size, layers_params):\n",
    "        \n",
    "        encoders.append(Dropout(l_param['dropout']))\n",
    "\n",
    "        # construct encoding layer \n",
    "        encoding_layer = Dense(\n",
    "            units = l_size,\n",
    "            activation=l_param['activation'],\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01),\n",
    "            bias_initializer=tf.keras.initializers.Zeros(),\n",
    "            kernel_regularizer=tf.keras.regularizers.L1L2(l_param['l1'], l_param['l2']),\n",
    "        )\n",
    "        # add encoding layer to the array of encoding layer\n",
    "        encoders.append(encoding_layer)\n",
    "       \n",
    "        # construct decoding layer as dense transpose of encoding layer \n",
    "        decoders.append(DenseTranspose(dense=encoding_layer))\n",
    "\n",
    "    # reverse decoders array \n",
    "    decoders = decoders[::-1]\n",
    "\n",
    "    # stack layers to form auto-encoder architecture\n",
    "    stacked_layers = inputs\n",
    "    for layer in encoders + decoders:\n",
    "        stacked_layers = layer(stacked_layers)\n",
    "\n",
    "    model = tf.keras.Model(inputs, stacked_layers)\n",
    "\n",
    "    opt = optimizer(learning_rate=learning_rate)\n",
    "    if metrics == None:\n",
    "        model.compile(loss=loss, optimizer=opt)\n",
    "    else:\n",
    "        model.compile(loss=loss, optimizer=opt, metrics=metrics)\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "def tmp(tup : Optional[Tuple[str,str]]= (\"a\", \"b\")):\n",
    "    print(tup[0], tup[1])\n",
    "tmp()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a b\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from typing import List, Tuple, Union, Optional\n",
    "\n",
    "def build_autoencoder(\n",
    "    input_dim : int,\n",
    "    min_dim : int,\n",
    "    width : List[int],\n",
    "    transposed_decoder : Optional[bool] = True,\n",
    "    activation  = \"tanh\",\n",
    "    loss = \"mse\",\n",
    "    optimizer = \"Adam\",\n",
    "    metrics = [\"mae\", \"acc\"],\n",
    "    name : Tuple[str, str] = ('encoder', 'decoder')\n",
    "    ) -> Tuple[tf.keras.Model, tf.keras.Model, tf.keras.Model]:\n",
    "    \"\"\"\n",
    "    Unfortunately, at the time of writing this code\n",
    "    python typing did not support all keras types:\n",
    "    https://github.com/tensorflow/tensorflow/issues/46337\n",
    "\n",
    "    If if it will be resolved, types above can be improved:\n",
    "        activation : Optional[Union[tf.keras.activations, str]] = \"tanh\",\n",
    "        loss : Optional[Union[tf.keras.losses, str]] = \"mse\",\n",
    "        optimizer : Optional[Union[str, tf.keras.optimizers] = \"Adam\",\n",
    "        metrics : Optional[List[Union[str, tf.keras.metrics], tf.keras.metrics, str] = [\"mae\", \"acc\"]\n",
    "\n",
    "    !!! my previous kernal initializer was: tf.keras.initializers.RandomNormal(stddev=0.01)\n",
    "    \"\"\"\n",
    "    \n",
    "    name_encoder = 'endocer' if name[0]==None else name[0] \n",
    "    name_decoder = 'decoder' if name[1]==None else name[1]\n",
    "\n",
    "    encoder_layers = [Input(shape=(input_dim, ), name = name_encoder + '_input')]\n",
    "    decoder_layers = [Input(shape=(min_dim, ), name = name_decoder + '_input')]\n",
    "\n",
    "    for layer_index, layer_dim in enumerate(width):\n",
    "        # construct encoder layer \n",
    "        encoder_layers.append(\n",
    "            Dense(\n",
    "                units = layer_dim,\n",
    "                activation=activation, \n",
    "                kernel_initializer=tf.keras.initializers.GlorotUniform(),\n",
    "                bias_initializer=tf.keras.initializers.Zeros(),\n",
    "                name = name_encoder + \"_{}\".format(layer_index)\n",
    "                )\n",
    "            )\n",
    "    encoder_layers.append(\n",
    "        Dense(\n",
    "            units = min_dim,\n",
    "            activation=activation, \n",
    "            kernel_initializer=tf.keras.initializers.GlorotUniform(),\n",
    "            bias_initializer=tf.keras.initializers.Zeros(),\n",
    "            name = name_encoder + \"_output\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if transposed_decoder:\n",
    "        for layer_index, layer in enumerate(encoder_layers[::-1][:-1]):\n",
    "            # construct decoder layer\n",
    "            decoder_layers.append(\n",
    "                DenseTranspose(\n",
    "                    dense = layer,\n",
    "                    name = name_decoder + \"_{}\".format(layer_index)\n",
    "                    )\n",
    "                )\n",
    "    else:\n",
    "        for layer_index, layer in enumerate(width[::-1]):\n",
    "            # construct decoder layer \n",
    "            decoder_layers.append(\n",
    "                Dense(\n",
    "                    units = layer.input_shape[-1],\n",
    "                    activation=activation, \n",
    "                    kernel_initializer=tf.keras.initializers.GlorotUniform(),\n",
    "                    bias_initializer=tf.keras.initializers.Zeros(),\n",
    "                    name = name_decoder + \"_{}\".format(layer_index)\n",
    "                    )\n",
    "                )\n",
    "        decoder_layers.append(\n",
    "            Dense(\n",
    "                units = input_dim,\n",
    "                activation=activation, \n",
    "                kernel_initializer=tf.keras.initializers.GlorotUniform(),\n",
    "                bias_initializer=tf.keras.initializers.Zeros(),\n",
    "                name = name_decoder + \"_{}\".format(layer_index)\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # stack encoder layers into encoder\n",
    "    encoder_input = encoder_layers[0]\n",
    "    x = encoder_input\n",
    "    for encoder_layer in encoder_layers[1:]:\n",
    "        x = encoder_layer(x)\n",
    "    \n",
    "    # build encoder\n",
    "    encoder = tf.keras.Model(encoder_input, x)\n",
    "    encoder.compile(loss = loss, optimizer = optimizer, metrics=metrics)\n",
    "\n",
    "    # stack decoder layers into decoder\n",
    "    decoder_input = decoder_layers[0]\n",
    "    z = decoder_input\n",
    "    for decoder_layer in decoder_layers[1:]:\n",
    "        z = decoder_layer(z)\n",
    "    \n",
    "    # build decoder\n",
    "    decoder = tf.keras.Model(decoder_input, z)\n",
    "    decoder.compile(loss = loss, optimizer = optimizer, metrics=metrics)\n",
    "\n",
    "    # interation over encoder/decoder is intended to keep consistant layer naming\n",
    "    autoencoder_input = encoder_layers[0]\n",
    "    y = autoencoder_input\n",
    "    for autoencoder_layer in encoder_layers[1:] + decoder_layers[1:]:\n",
    "        y = autoencoder_layer(y)\n",
    "\n",
    "    # build autoencoder\n",
    "    autoencoder = tf.keras.Model(autoencoder_input, y)\n",
    "    autoencoder.compile(loss = loss, optimizer = optimizer, metrics=metrics)\n",
    "\n",
    "    return autoencoder, encoder, decoder, encoder_layers, decoder_layers\n",
    "\n",
    "a, e, d, el, dl = build_autoencoder(32, 8, [24, 16])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "e.layers"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x7f85b91f8a20>,\n",
       " <keras.layers.core.Dense at 0x7f85b91f9198>,\n",
       " <keras.layers.core.Dense at 0x7f85b91f9470>,\n",
       " <keras.layers.core.Dense at 0x7f85b91f9710>]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "e.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "encoder_0 (Dense)            (None, 24)                792       \n",
      "_________________________________________________________________\n",
      "encoder_1 (Dense)            (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "encoder_output (Dense)       (None, 8)                 136       \n",
      "=================================================================\n",
      "Total params: 1,328\n",
      "Trainable params: 1,328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "input_l = Input(shape=(mod.layers[-1].output_shape[-1], ), name = '_input')\n",
    "x = DenseTranspose(dense = mod.layers[-1])(input_l)\n",
    "tf.keras.Model(input_l, x)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x7f7e1f2b7470>"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"Good autoencoder with nice dict args\"\"\"\n",
    "def build_autoencoder(\n",
    "    encoding_layers_size = [32, 28], \n",
    "    layers_params = [{'l1' : 0.0001, 'l2' : 0.0, 'dropout' : 0.0}, {}],\n",
    "    layers_default_params = {'l1' : 0.0001, 'l2' : 0.0, 'dropout' : 0.0, 'activation' : \"tanh\"},\n",
    "    loss = \"mse\",\n",
    "    optimizer =  tf.keras.optimizers.Adam,\n",
    "    learning_rate = 0.0001,\n",
    "    metrics = None\n",
    "    ):\n",
    "\n",
    "    # initialize input layer \n",
    "    input_size = encoding_layers_size[0]\n",
    "    inputs = Input(shape=(input_size,))\n",
    "\n",
    "    # add empty dicionaries to layers_params to corecponds to the encoding_layers_size size\n",
    "    for _ in range(len(encoding_layers_size) - len(layers_params)):\n",
    "        layers_params.append({})\n",
    "\n",
    "    # remove firs instance \n",
    "    encoding_layers_size = encoding_layers_size[1:]\n",
    "\n",
    "    # fill missing values in layers_params using default values \n",
    "    layers_params = [{**layers_default_params, **x} for x in layers_params]\n",
    "    \n",
    "    #initalize arrays to hold encoding and decoding layers \n",
    "    encoders = []\n",
    "    decoders = []\n",
    "\n",
    "    for l_size, l_param in zip(encoding_layers_size, layers_params):\n",
    "        \n",
    "        # if dropout is different than 0 then add dropout layer\n",
    "        #if l_param['dropout'] > 0.0:\n",
    "        encoders.append(Dropout(l_param['dropout']))\n",
    "\n",
    "\n",
    "        # construct encoding layer \n",
    "        encoding_layer = Dense(\n",
    "            units = l_size,\n",
    "            activation=l_param['activation'],\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01),\n",
    "            bias_initializer=tf.keras.initializers.Zeros(),\n",
    "            kernel_regularizer=tf.keras.regularizers.L1L2(l_param['l1'], l_param['l2']),\n",
    "        )\n",
    "        # add encoding layer to the array of encoding layer\n",
    "        encoders.append(encoding_layer)\n",
    "       \n",
    "        # construct decoding layer as dense transpose of encoding layer \n",
    "        decoders.append(DenseTranspose(dense=encoding_layer))\n",
    "\n",
    "    # remove dropout layer after the bottlenect layer\n",
    "    #if(type(encoders[-1]) == type(Dropout(0.0))):\n",
    "    #    encoders.pop()\n",
    "\n",
    "    # reverse decoders array \n",
    "    decoders = decoders[::-1]\n",
    "\n",
    "    # stack layers to form auto-encoder architecture\n",
    "    stacked_layers = inputs\n",
    "    for layer in encoders + decoders:\n",
    "        stacked_layers = layer(stacked_layers)\n",
    "\n",
    "    model = tf.keras.Model(inputs, stacked_layers)\n",
    "\n",
    "    opt = optimizer(learning_rate=learning_rate)\n",
    "    if metrics == None:\n",
    "        model.compile(loss=loss, optimizer=opt)\n",
    "    else:\n",
    "        model.compile(loss=loss, optimizer=opt, metrics=metrics)\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "\"\"\"Good model building\"\"\"\n",
    "\n",
    "from typing import List, Tuple, Union, Optional\n",
    "\n",
    "def build_autoencoder(\n",
    "    input_dim : int,\n",
    "    min_dim : int,\n",
    "    width : List[int],\n",
    "    transposed_decoder : Optional[bool] = True,\n",
    "    activation  = \"tanh\",\n",
    "    loss = \"mse\",\n",
    "    optimizer = \"Adam\",\n",
    "    metrics = [\"mae\", \"acc\"],\n",
    "    name : Tuple[str, str] = ('encoder', 'decoder')\n",
    "    ) -> Tuple[tf.keras.Model, tf.keras.Model, tf.keras.Model]:\n",
    "    \"\"\"\n",
    "    Unfortunately, at the time of writing this code\n",
    "    python typing did not support all keras types:\n",
    "    https://github.com/tensorflow/tensorflow/issues/46337\n",
    "\n",
    "    If if it will be resolved, types above can be improved:\n",
    "        activation : Optional[Union[tf.keras.activations, str]] = \"tanh\",\n",
    "        loss : Optional[Union[tf.keras.losses, str]] = \"mse\",\n",
    "        optimizer : Optional[Union[str, tf.keras.optimizers] = \"Adam\",\n",
    "        metrics : Optional[List[Union[str, tf.keras.metrics], tf.keras.metrics, str] = [\"mae\", \"acc\"]\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    name_encoder = 'endocer' if name[0]==None else name[0] \n",
    "    name_decoder = 'decoder' if name[1]==None else name[1]\n",
    "\n",
    "    encoder_layers = [Input(shape=(input_dim, ), name = name_encoder + '_input')]\n",
    "    decoder_layers = [Input(shape=(min_dim, ), name = name_decoder + '_input')]\n",
    "\n",
    "    for layer_index, layer_dim in enumerate(width):\n",
    "        # construct encoder layer \n",
    "        encoder_layers.append(\n",
    "            Dense(\n",
    "                units = layer_dim,\n",
    "                activation=activation, \n",
    "                kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01),\n",
    "                bias_initializer=tf.keras.initializers.Zeros(),\n",
    "                name = name_encoder + \"_{}\".format(layer_index)\n",
    "                )\n",
    "            )\n",
    "    encoder_layers.append(\n",
    "        Dense(\n",
    "            units = min_dim,\n",
    "            activation=activation, \n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01),\n",
    "            bias_initializer=tf.keras.initializers.Zeros(),\n",
    "            name = name_encoder + \"_output\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if transposed_decoder:\n",
    "        for layer_index, layer in enumerate(encoder_layers[::-1][:-1]):\n",
    "            # construct decoder layer\n",
    "            decoder_layers.append(\n",
    "                DenseTranspose(\n",
    "                    dense = layer,\n",
    "                    name = name_decoder + \"_{}\".format(layer_index)\n",
    "                    )\n",
    "                )\n",
    "    else:\n",
    "        for layer_index, layer in enumerate(width[::-1]):\n",
    "            # construct decoder layer \n",
    "            decoder_layers.append(\n",
    "                Dense(\n",
    "                    units = layer.input_shape[-1],\n",
    "                    activation=activation, \n",
    "                    kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01),\n",
    "                    bias_initializer=tf.keras.initializers.Zeros(),\n",
    "                    name = name_decoder + \"_{}\".format(layer_index)\n",
    "                    )\n",
    "                )\n",
    "        decoder_layers.append(\n",
    "            Dense(\n",
    "                units = input_dim,\n",
    "                activation=activation, \n",
    "                kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01),\n",
    "                bias_initializer=tf.keras.initializers.Zeros(),\n",
    "                name = name_decoder + \"_{}\".format(layer_index)\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # stack encoder layers into encoder\n",
    "    encoder_input = encoder_layers[0]\n",
    "    x = encoder_input\n",
    "    for encoder_layer in encoder_layers[1:]:\n",
    "        x = encoder_layer(x)\n",
    "    \n",
    "    # build encoder\n",
    "    encoder = tf.keras.Model(encoder_input, x)\n",
    "    encoder.compile(loss = loss, optimizer = optimizer, metrics=metrics)\n",
    "\n",
    "    # stack decoder layers into decoder\n",
    "    decoder_input = decoder_layers[0]\n",
    "    z = decoder_input\n",
    "    for decoder_layer in decoder_layers[1:]:\n",
    "        z = decoder_layer(z)\n",
    "    \n",
    "    # build decoder\n",
    "    decoder = tf.keras.Model(decoder_input, z)\n",
    "    decoder.compile(loss = loss, optimizer = optimizer, metrics=metrics)\n",
    "\n",
    "    # interation over encoder/decoder is intended to keep consistant layer naming\n",
    "    autoencoder_input = encoder_layers[0]\n",
    "    y = autoencoder_input\n",
    "    for autoencoder_layer in encoder_layers[1:] + decoder_layers[1:]:\n",
    "        y = autoencoder_layer(y)\n",
    "\n",
    "    # build autoencoder\n",
    "    autoencoder = tf.keras.Model(autoencoder_input, y)\n",
    "    autoencoder.compile(loss = loss, optimizer = optimizer, metrics=metrics)\n",
    "\n",
    "    return autoencoder, encoder, decoder\n",
    "\n",
    "a, e, d = build_autoencoder(32, 8, [24, 16])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "def build_decoder_from_encoder(encoder: tf.keras.models.Model, name : str = \"decoder\"):\n",
    "    # reversed list of encoder layers\n",
    "    decoder_layers = encoder.layers[::-1]\n",
    "    input_dim = decoder_layers[0].output_shape[-1]\n",
    "    print(decoder_layers)\n",
    "\n",
    "    input_layer = Input(shape=(None, input_dim), name = name + '_input')\n",
    "\n",
    "    x = input_layer\n",
    "    for layer_index, decoder_layer in enumerate(decoder_layers):\n",
    "        #  construct decoding layer that is transposed encoding layer\n",
    "        print(decoder_layer.output_shape)\n",
    "        x = DenseTranspose(\n",
    "            decoder_layer\n",
    "        )(x)\n",
    "\n",
    "    output_layer = DenseTranspose(\n",
    "        decoder_layers[-1]\n",
    "    )(x)\n",
    "\n",
    "    model = tf.keras.Model(input_layer, output_layer)\n",
    "    return model \n",
    "build_decoder_from_encoder(mod)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[<keras.layers.core.Dense object at 0x7f7e1f55ab00>, <keras.layers.core.Dense object at 0x7f7e1f55a3c8>, <keras.layers.core.Dense object at 0x7f7e1f5dbc18>, <keras.engine.input_layer.InputLayer object at 0x7f7e1f5db7b8>]\n",
      "(None, 8)\n",
      "(None, 16)\n",
      "(None, 24)\n",
      "[(None, 32)]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Dimension value must be integer or None or have an __index__ method, got value '(None, 32)' with type '<class 'tuple'>'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-ed461f61129b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mbuild_decoder_from_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-ed461f61129b>\u001b[0m in \u001b[0;36mbuild_decoder_from_encoder\u001b[0;34m(encoder, name)\u001b[0m\n\u001b[1;32m     13\u001b[0m         x = DenseTranspose(\n\u001b[1;32m     14\u001b[0m             \u001b[0mdecoder_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         )(x)\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     output_layer = DenseTranspose(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 977\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1113\u001b[0m       \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m       outputs = self._keras_tensor_symbolic_call(\n\u001b[0;32m-> 1115\u001b[0;31m           inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    884\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m           \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2657\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m       \u001b[0;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m       \u001b[0;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SindyAutoencoders/examples/lorenz/shallowNet/shallowNet.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, batch_input_shape)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDenseTranspose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_input_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"bias\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"zeros\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_input_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m         caching_device=caching_device)\n\u001b[0m\u001b[1;32m    664\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m       \u001b[0;31m# TODO(fchollet): in the future, this should be handled at the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    816\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36mmake_variable\u001b[0;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mvariable_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m   \u001b[0mvariable_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0muse_resource\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    763\u001b[0m     \"\"\"\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Most common case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    763\u001b[0m     \"\"\"\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Most common case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    207\u001b[0m             TypeError(\"Dimension value must be integer or None or have \"\n\u001b[1;32m    208\u001b[0m                       \u001b[0;34m\"an __index__ method, got value '{0!r}' with type '{1!r}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                       .format(value, type(value))), None)\n\u001b[0m\u001b[1;32m    210\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dimension %d must be >= 0\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Dimension value must be integer or None or have an __index__ method, got value '(None, 32)' with type '<class 'tuple'>'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-9e590dc3afd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbuild_decoder_from_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-046039dc3dcf>\u001b[0m in \u001b[0;36mbuild_decoder_from_encoder\u001b[0;34m(encoder, name)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m#  construct decoding layer that is transposed encoding layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         x = DenseTranspose(\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "def build_autoencoder(\n",
    "    encoding_layers_size = [32, 28], \n",
    "    layers_params = [{'l1' : 0.0001, 'l2' : 0.0, 'dropout' : 0.0}, {}],\n",
    "    layers_default_params = {'l1' : 0.0001, 'l2' : 0.0, 'dropout' : 0.0, 'activation' : \"tanh\"},\n",
    "    loss = \"mse\",\n",
    "    optimizer =  tf.keras.optimizers.Adam,\n",
    "    learning_rate = 0.0001,\n",
    "    metrics = None\n",
    "    ) -> tf.keras.Model:\n",
    "\n",
    "    # initialize input layer \n",
    "    input_size = encoding_layers_size[0]\n",
    "    inputs = Input(shape=(input_size,))\n",
    "\n",
    "    # add empty dicionaries to layers_params to corecponds to the encoding_layers_size size\n",
    "    for _ in range(len(encoding_layers_size) - len(layers_params)):\n",
    "        layers_params.append({})\n",
    "\n",
    "    # remove firs instance \n",
    "    encoding_layers_size = encoding_layers_size[1:]\n",
    "\n",
    "    # fill missing values in layers_params using default values \n",
    "    layers_params = [{**layers_default_params, **x} for x in layers_params]\n",
    "    \n",
    "    #initalize arrays to hold encoding and decoding layers \n",
    "    encoders = []\n",
    "    decoders = []\n",
    "\n",
    "    for l_size, l_param in zip(encoding_layers_size, layers_params):\n",
    "        \n",
    "        encoders.append(Dropout(l_param['dropout']))\n",
    "\n",
    "        # construct encoding layer \n",
    "        encoding_layer = Dense(\n",
    "            units = l_size,\n",
    "            activation=l_param['activation'],\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01),\n",
    "            bias_initializer=tf.keras.initializers.Zeros(),\n",
    "            kernel_regularizer=tf.keras.regularizers.L1L2(l_param['l1'], l_param['l2']),\n",
    "        )\n",
    "        # add encoding layer to the array of encoding layer\n",
    "        encoders.append(encoding_layer)\n",
    "       \n",
    "        # construct decoding layer as dense transpose of encoding layer \n",
    "        decoders.append(DenseTranspose(dense=encoding_layer))\n",
    "\n",
    "    # reverse decoders array \n",
    "    decoders = decoders[::-1]\n",
    "\n",
    "    # stack layers to form auto-encoder architecture\n",
    "    stacked_layers = inputs\n",
    "    for layer in encoders + decoders:\n",
    "        stacked_layers = layer(stacked_layers)\n",
    "\n",
    "    model = tf.keras.Model(inputs, stacked_layers)\n",
    "\n",
    "    opt = optimizer(learning_rate=learning_rate)\n",
    "    if metrics == None:\n",
    "        model.compile(loss=loss, optimizer=opt)\n",
    "    else:\n",
    "        model.compile(loss=loss, optimizer=opt, metrics=metrics)\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "build_autoencoder().summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 28)                924       \n",
      "_________________________________________________________________\n",
      "dense_transpose (DenseTransp (None, 32)                956       \n",
      "=================================================================\n",
      "Total params: 956\n",
      "Trainable params: 956\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "def define_loss(network, params):\n",
    "    \"\"\"\n",
    "    Create the loss functions.\n",
    "\n",
    "    Arguments:\n",
    "        network - Dictionary object containing the elements of the network architecture.\n",
    "        This will be the output of the full_network() function.\n",
    "    \"\"\"\n",
    "    x = network['x']\n",
    "    x_decode = network['x_decode']\n",
    "    if params['model_order'] == 1:\n",
    "        dz = network['dz']\n",
    "        dz_predict = network['dz_predict']\n",
    "        dx = network['dx']\n",
    "        dx_decode = network['dx_decode']\n",
    "    else:\n",
    "        ddz = network['ddz']\n",
    "        ddz_predict = network['ddz_predict']\n",
    "        ddx = network['ddx']\n",
    "        ddx_decode = network['ddx_decode']\n",
    "    sindy_coefficients = params['coefficient_mask']*network['sindy_coefficients']\n",
    "\n",
    "    losses = {}\n",
    "    losses['decoder'] = tf.math.reduce_mean((x - x_decode)**2)\n",
    "    if params['model_order'] == 1:\n",
    "        losses['sindy_z'] = tf.math.reduce_mean((dz - dz_predict)**2)\n",
    "        losses['sindy_x'] = tf.math.reduce_mean((dx - dx_decode)**2)\n",
    "    else:\n",
    "        losses['sindy_z'] = tf.math.reduce_mean((ddz - ddz_predict)**2)\n",
    "        losses['sindy_x'] = tf.math.reduce_mean((ddx - ddx_decode)**2)\n",
    "    losses['sindy_regularization'] = tf.math.reduce_mean(tf.math.abs(sindy_coefficients))\n",
    "    loss = params['loss_weight_decoder'] * losses['decoder'] \\\n",
    "           + params['loss_weight_sindy_z'] * losses['sindy_z'] \\\n",
    "           + params['loss_weight_sindy_x'] * losses['sindy_x'] \\\n",
    "           + params['loss_weight_sindy_regularization'] * losses['sindy_regularization']\n",
    "\n",
    "    loss_refinement = params['loss_weight_decoder'] * losses['decoder'] \\\n",
    "                      + params['loss_weight_sindy_z'] * losses['sindy_z'] \\\n",
    "                      + params['loss_weight_sindy_x'] * losses['sindy_x']\n",
    "\n",
    "    return loss, losses, loss_refinement"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "input_size = 32\n",
    "set_size = 100\n",
    "compression = 0.8\n",
    "reg_cof = 1e-3\n",
    "epochs = 40\n",
    "batch = 4\n",
    "\n",
    "mod = build_autoencoder(\n",
    "    encoding_layers_size = [32, 25], \n",
    "    layers_params = [{'l1' : 1e-3, 'l2' : 0.0, 'dropout' : 0.0}, {}],\n",
    "    layers_default_params = {'l1' : 0.0001, 'l2' : 0.0, 'dropout' : 0.0, 'activation' : \"tanh\"},\n",
    "    loss = \"mse\",\n",
    "    optimizer =  tf.keras.optimizers.Adam,\n",
    "    learning_rate = 0.01,\n",
    "    metrics = None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "mod.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 25)                825       \n",
      "_________________________________________________________________\n",
      "dense_transpose_1 (DenseTran (None, 32)                857       \n",
      "=================================================================\n",
      "Total params: 857\n",
      "Trainable params: 857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "\n",
    "def build_network_layers(input, input_dim, output_dim, widths, activation, name):\n",
    "    \"\"\"\n",
    "    Construct one portion of the network (either encoder or decoder).\n",
    "\n",
    "    Arguments:\n",
    "        input - 2D tensorflow array, input to the network (shape is [?,input_dim])\n",
    "        input_dim - Integer, number of state variables in the input to the first layer\n",
    "        output_dim - Integer, number of state variables to output from the final layer\n",
    "        widths - List of integers representing how many units are in each network layer\n",
    "        activation - Tensorflow function to be used as the activation function at each layer\n",
    "        name - String, prefix to be used in naming the tensorflow variables\n",
    "\n",
    "    Returns:\n",
    "        input - Tensorflow array, output of the network layers (shape is [?,output_dim])\n",
    "        weights - List of tensorflow arrays containing the network weights\n",
    "        biases - List of tensorflow arrays containing the network biases\n",
    "    \"\"\"\n",
    "    weights = []\n",
    "    biases = []\n",
    "    last_width=input_dim\n",
    "    for i,n_units in enumerate(widths):\n",
    "        W = tf.get_variable(name+'_W'+str(i), shape=[last_width,n_units],\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable(name+'_b'+str(i), shape=[n_units],\n",
    "            initializer=tf.constant_initializer(0.0))\n",
    "        input = tf.matmul(input, W) + b\n",
    "        if activation is not None:\n",
    "            input = activation(input)\n",
    "        last_width = n_units\n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "    W = tf.get_variable(name+'_W'+str(len(widths)), shape=[last_width,output_dim],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b = tf.get_variable(name+'_b'+str(len(widths)), shape=[output_dim],\n",
    "        initializer=tf.constant_initializer(0.0))\n",
    "    input = tf.matmul(input,W) + b\n",
    "    weights.append(W)\n",
    "    biases.append(b)\n",
    "    return input, weights, biases"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def sindy_library_tf(z, latent_dim, poly_order, include_sine=False):\n",
    "    \"\"\"\n",
    "    Build the SINDy library.\n",
    "\n",
    "    Arguments:\n",
    "        z - 2D tensorflow array of the snapshots on which to build the library. Shape is number of\n",
    "        time points by the number of state variables.\n",
    "        latent_dim - Integer, number of state variable in z.\n",
    "        poly_order - Integer, polynomial order to which to build the library. Max value is 5.\n",
    "        include_sine - Boolean, whether or not to include sine terms in the library. Default False.\n",
    "\n",
    "    Returns:\n",
    "        2D tensorflow array containing the constructed library. Shape is number of time points by\n",
    "        number of library functions. The number of library functions is determined by the number\n",
    "        of state variables of the input, the polynomial order, and whether or not sines are included.\n",
    "    \"\"\"\n",
    "\n",
    "    # add constant term to the set of equation\n",
    "    library = [tf.ones(tf.shape(z)[0])]\n",
    "\n",
    "    # add: x, y, z\n",
    "    for i in range(latent_dim):\n",
    "        library.append(z[:,i])\n",
    "\n",
    "    # add: xx + zz + yy + xy + xz + yz\n",
    "    if poly_order > 1:\n",
    "        print(\"call poly 1\")\n",
    "        for i in range(latent_dim):\n",
    "            for j in range(i,latent_dim):\n",
    "                library.append(tf.math.multiply(z[:,i], z[:,j]))\n",
    "\n",
    "    # anologically to the comment above\n",
    "    if poly_order > 2:\n",
    "        print(\"call poly 2\")\n",
    "        for i in range(latent_dim):\n",
    "            for j in range(i,latent_dim):\n",
    "                for k in range(j,latent_dim):\n",
    "                    library.append(z[:,i]*z[:,j]*z[:,k])\n",
    "\n",
    "    # anologically to the comment above\n",
    "    if poly_order > 3:\n",
    "        print(\"call poly 3\")\n",
    "        for i in range(latent_dim):\n",
    "            for j in range(i,latent_dim):\n",
    "                for k in range(j,latent_dim):\n",
    "                    for p in range(k,latent_dim):\n",
    "                        library.append(z[:,i]*z[:,j]*z[:,k]*z[:,p])\n",
    "    \n",
    "    # anologically to the comment above\n",
    "    if poly_order > 4:\n",
    "        for i in range(latent_dim):\n",
    "            for j in range(i,latent_dim):\n",
    "                for k in range(j,latent_dim):\n",
    "                    for p in range(k,latent_dim):\n",
    "                        for q in range(p,latent_dim):\n",
    "                            library.append(z[:,i]*z[:,j]*z[:,k]*z[:,p]*z[:,q])\n",
    "    \n",
    "    # add: sin(x), sin(y), sin(z)\n",
    "    if include_sine:\n",
    "        for i in range(latent_dim):\n",
    "            library.append(tf.sin(z[:,i]))\n",
    "\n",
    "    return tf.stack(library, axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}